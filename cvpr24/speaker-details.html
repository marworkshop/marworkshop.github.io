<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>MAR 2024 - Multimodal Algorithmic Reasoning</title>
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <meta content="" name="keywords">
  <meta content="" name="description">

  <!-- Favicons -->
  <link href="img/cvpr24.png" rel="icon">
  <link href="img/cvpr24.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,700,700i|Raleway:300,400,500,700,800" rel="stylesheet">

  <!-- Bootstrap CSS File -->
  <link href="lib/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Libraries CSS Files -->
  <link href="lib/font-awesome/css/font-awesome.min.css" rel="stylesheet">
  <link href="lib/animate/animate.min.css" rel="stylesheet">
  <link href="lib/venobox/venobox.css" rel="stylesheet">
  <link href="lib/owlcarousel/assets/owl.carousel.min.css" rel="stylesheet">

  <!-- Main Stylesheet File -->
  <link href="css/style.css" rel="stylesheet">

  <!-- =======================================================
    Theme Name: TheEvent
    Theme URL: https://bootstrapmade.com/theevent-conference-event-bootstrap-template/
    Author: BootstrapMade.com
    License: https://bootstrapmade.com/license/
  ======================================================= -->
</head>

<body>

  <!--==========================
    Header
  ============================-->
  <header id="header" class="header-fixed">
    <div class="container">

      <div id="logo" class="pull-left">
        <!-- Uncomment below if you prefer to use a text logo -->
        <!-- <h1><a href="#main">C<span>o</span>nf</a></h1>-->
        <!-- <a href="index.html#intro" class="scrollto"><img src="img/logo.png" alt="" title=""></a> -->
        <a href="index.html#intro" class="scrollto"><p style="font-size:30px;color:white">MAR 2024</p></a>
      </div>

      <nav id="nav-menu-container">
        <ul class="nav-menu">
          <li class="menu-active"><a href="index.html#intro">Home</a></li>
          <li><a href="index.html#about">About</a></li>
          <li><a href="index.html#speakers">Speakers</a></li>
          <li><a href="index.html#schedule">Schedule</a></li>
		  <li><a href="index.html#submission">Submission</a></li>
		  <!--<li><a href="index.html#accepted_work">Accepted Work</a></li>-->
          <li><a href="index.html#venue">Venue</a></li>
          <!-- <li><a href="#hotels">Hotels</a></li> -->
          <!-- <li><a href="#gallery">Gallery</a></li> -->
          <li><a href="index.html#supporters">Sponsor</a></li>
          <!-- <li><a href="#contact">Contact</a></li> -->
          <li><a href="index.html#organizers">Contact</a></li>
		  <li><a href="https://wvlar.github.io/iccv23/">Past Workshop</a></li>
		  <li><a class="btn navbar-btn mx-2 text-white btn-outline-light" href="https://cmt3.research.microsoft.com/MAR2024" target="_blank" rel="noopener noreferrer">CMT</a></li>
		  <!-- <li><a class="btn navbar-btn mx-2 text-white btn-outline-light" href="https://eval.ai/web/challenges/challenge-page/2088/overview" target="_blank" rel="noopener noreferrer">Challenge</a></li> -->
		  <!-- <li><a class="btn navbar-btn mx-2 text-white btn-outline-light" href="https://smartdataset.github.io/smart101/" target="_blank" rel="noopener noreferrer">SMART-101</a></li> -->
          <!-- <li><a href="../index.html#VASBSD_series">VASBSD Series</a></li> -->
          <!-- <li class="buy-tickets"><a href="#buy-tickets">Buy Tickets</a></li> -->
        </ul>
      </nav><!-- #nav-menu-container -->
    </div>
  </header><!-- #header -->

  <main id="main" class="main-page">

    <!--==========================
      Speaker Details Section
    ============================-->
    <section id="speakers-details" class="wow fadeIn">
      <div class="container">
        <div class="section-header">
          <h2>Speaker Details</h2>
          <p>[more info about the speakers will be announced here]</p>
        </div>
        
        <div class="row">
          <div class="col-md-6">
            <a href="https://petar-v.com/" target="_blank" rel="noopener noreferrer"><img src="img/speakers/Petar_Velickovic.jpg" alt="Petar Velickovic" class="img-fluid" style="width:100%;padding-bottom: 0px"></a>
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Petar_Velickovic"><a href="https://petar-v.com/" target="_blank" rel="noopener noreferrer">Petar Veličković</a></h2>
              <p>Google DeepMind</p>
              <div class="social"> 
                <!--
                <a href=""><i class="fa fa-twitter"></i></a>
                <a href=""><i class="fa fa-facebook"></i></a>
                <a href=""><i class="fa fa-google-plus"></i></a>
                <a href=""><i class="fa fa-linkedin"></i></a>
				-->
              </div>
              <p>Bio:<br>Dr. Petar Veličković is a Staff Research Scientist at <a href="https://www.deepmind.com/" target="_blank" rel="noopener noreferrer">Google DeepMind</a>, Affiliated Lecturer at the <a href="https://www.cam.ac.uk/" target="_blank" rel="noopener noreferrer">University of Cambridge</a>, and an Associate of <a href="https://www.clarehall.cam.ac.uk/" target="_blank" rel="noopener noreferrer">Clare Hall, Cambridge</a>. He holds a PhD in Computer Science from the <a href="https://www.cam.ac.uk/" target="_blank" rel="noopener noreferrer">University of Cambridge</a> (<a href="https://www.trin.cam.ac.uk/" target="_blank" rel="noopener noreferrer">Trinity College</a>), obtained under the supervision of <a href="https://www.cst.cam.ac.uk/~pl219" target="_blank" rel="noopener noreferrer">Pietro Liò</a>. His research concerns <a href="https://www.youtube.com/watch?v=9cxhvQK9ALQ" target="_blank" rel="noopener noreferrer">geometric deep learning</a>—devising neural network architectures that respect the invariances and symmetries in data (a topic he has co-written a <a href="https://geometricdeeplearning.com/" target="_blank" rel="noopener noreferrer">proto-book</a> about). For his contributions, he is recognised as an <a href="https://ellis.eu/" target="_blank" rel="noopener noreferrer">ELLIS</a> Scholar in the <a href="https://ellis.eu/programs/geometric-deep-learning" target="_blank" rel="noopener noreferrer">Geometric Deep Learning Program</a>. Particularly, he focuses on <a href="https://www.youtube.com/watch?v=uF53xsT7mjc" target="_blank" rel="noopener noreferrer">graph representation learning</a> and its applications in <a href="https://www.youtube.com/watch?v=U2vybDdoDAQ" target="_blank" rel="noopener noreferrer">algorithmic reasoning</a> (featured in <a href="https://venturebeat.com/2021/09/10/deepmind-aims-to-marry-deep-learning-and-classic-algorithms/" target="_blank" rel="noopener noreferrer">Venture</a><a href="https://venturebeat.com/2021/10/12/deepmind-is-developing-one-algorithm-to-rule-them-all/" target="_blank" rel="noopener noreferrer">Beat</a>). He is the first author of <a href="http://petar-v.com/GAT/" target="_blank" rel="noopener noreferrer">Graph Attention Networks</a>—a popular convolutional layer for graphs—and <a href="https://openreview.net/forum?id=rklz9iAcKQ" target="_blank" rel="noopener noreferrer">Deep Graph Infomax</a>—a popular self-supervised learning pipeline for graphs (featured in <a href="https://www.zdnet.com/article/google-brain-microsoft-plumb-the-mysteries-of-networks-with-ai/" target="_blank" rel="noopener noreferrer">ZDNet</a>). His research has been used in <a href="https://deepmind.com/blog/article/traffic-prediction-with-advanced-graph-neural-networks" target="_blank" rel="noopener noreferrer">substantially improving travel-time predictions</a> in <a href="https://blog.google/products/maps/google-maps-101-how-ai-helps-predict-traffic-and-determine-routes/" target="_blank" rel="noopener noreferrer">Google Maps</a> (featured in the <a href="https://www.cnbc.com/2020/09/03/covid-19-forced-google-maps-to-change-how-it-predicts-traffic.html" target="_blank" rel="noopener noreferrer">CNBC</a>, <a href="https://www.engadget.com/google-maps-deep-mind-ai-accuracy-140005698.html" target="_blank" rel="noopener noreferrer">Endgadget</a>, <a href="https://deepmind.google/technologies/synthid/https://venturebeat.com/2020/09/03/deepmind-claims-its-ai-improved-google-maps-travel-time-estimates-by-up-to-50/oreferrer">VentureBeat</a>, <a href="https://www.cnet.com/news/heres-how-google-maps-uses-ai-to-predict-traffic-and-calculate-routes/" target="_blank" rel="noopener noreferrer">CNET</a>, the <a href="https://www.theverge.com/2020/9/3/21419632/how-google-maps-predicts-traffic-eta-ai-machine-learning-deepmind" target="_blank" rel="noopener noreferrer">Verge</a> and <a href="https://www.zdnet.com/article/google-maps-and-deepmind-enhance-ai-capabilities-to-improve-route-calculations/" target="_blank" rel="noopener noreferrer">ZDNet</a>), and <a href="https://www.nature.com/articles/s41586-021-04086-x" target="_blank" rel="noopener noreferrer">guiding intuition of mathematicians</a> towards new top-tier <a href="https://arxiv.org/abs/2111.15161" target="_blank" rel="noopener noreferrer">theorems and conjectures</a> (featured in <a href="https://www.nature.com/articles/d41586-021-03593-1" target="_blank" rel="noopener noreferrer">Nature</a>, <a href="https://www.science.org/content/blog-post/ai-aid-human-intuition" target="_blank" rel="noopener noreferrer">Science</a>, <a href="https://www.quantamagazine.org/deepmind-machine-learning-becomes-a-mathematical-collaborator-20220215/" target="_blank" rel="noopener noreferrer">Quanta Magazine</a>, <a href="https://www.newscientist.com/article/2299564-deepmind-ai-collaborates-with-humans-on-two-mathematical-breakthroughs/" target="_blank" rel="noopener noreferrer">New Scientist</a>, <a href="https://www.independent.co.uk/life-style/gadgets-and-tech/ai-artificial-intelligence-maths-deepmind-b1967817.html" target="_blank" rel="noopener noreferrer">The Independent</a>, <a href="https://news.sky.com/story/mathematicians-hail-breakthrough-in-using-ai-to-suggest-new-theorems-12483934" target="_blank" rel="noopener noreferrer">Sky News</a>, <a href="https://www.thetimes.co.uk/article/deepminds-artificial-intelligence-software-helps-mathematicians-pinpoint-patterns-3c5cwmmdt" target="_blank" rel="noopener noreferrer">The Sunday Times</a>, <a href="https://www.repubblica.it/tecnologia/2022/07/04/news/intuizione_aumentata_cosi_lintelligenza_artificiale_aiuta_i_matematici_nelle_loro_scoperte-354320142/" target="_blank" rel="noopener noreferrer">la Repubblica</a> and <a href="https://theconversation.com/mathematical-discoveries-take-intuition-and-creativity-and-now-a-little-help-from-ai-172900" target="_blank" rel="noopener noreferrer">The Conversation</a>).</p>
              <p>Keynote Title:<br>TBD.</p>
			  <p>Keynote Abstract:<br>TBD.</p> 
			  <!-- <a href="">[Talk Video]</a> -->

            </div>
          </div>  
        </div>

        <p></p>
		
        <div class="row">
          <div class="col-md-6">
            <a href="https://cocosci.princeton.edu/tom/index.php" target="_blank" rel="noopener noreferrer"><img src="img/speakers/Tom_Griffiths.jpg" alt="Tom Griffiths" class="img-fluid" style="width:100%;padding-bottom: 0px"></a>
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Tom_Griffiths"><a href="https://cocosci.princeton.edu/tom/index.php" target="_blank" rel="noopener noreferrer">Tom Griffiths</a></h2>
              <p>Princeton University</p>
              <div class="social">
				<!--
				<a href=""><i class="fa fa-linkedin"></i></a>
                <a href=""><i class="fa fa-twitter"></i></a>
                <a href=""><i class="fa fa-facebook"></i></a>
                <a href=""><i class="fa fa-google-plus"></i></a>
                -->
              </div>
              <p>Bio:<br>Dr. Tom Griffiths is the Henry R. Luce Professor of Information Technology, Consciousness and Culture in the Departments of Psychology and Computer Science at Princeton University. His research explores connections between human and machine learning, using ideas from statistics and artificial intelligence to understand how people solve the challenging computational problems they encounter in everyday life. Tom completed his PhD in Psychology at Stanford University in 2005, and taught at Brown University and the University of California, Berkeley before moving to Princeton. He has received awards for his research from organizations ranging from the American Psychological Association to the National Academy of Sciences and is a co-author of the book Algorithms to Live By, introducing ideas from computer science and cognitive science to a general audience.</p>
              <p>Keynote Title:<br>Abstraction in Humans and Machines.</p>
			  <p>Keynote Abstract:<br>Machine learning has made great strides in creating systems that demonstrate high performance on tasks that were previously only performed by humans. However, are the solutions that they find comparable to those that humans use? In this talk I will summarize recent work analyzing the behavior of deep neural networks performing multimodal tasks that shows that these models failed to capture important abstractions that guide human performance in those tasks. I will also present some ideas on how we can better guide systems towards developing those abstractions.</p> 
			  <!-- <a href="">[Talk Video]</a> -->
            </div>
          </div>  
        </div>
		
		<p></p>

        <div class="row">
          <div class="col-md-6">
            <a href="https://research.google/people/pushmeet-kohli/" target="_blank" rel="noopener noreferrer"><img src="img/speakers/Pushmeet_Kohli.jpg" alt="Pushmeet_Kohli" class="img-fluid" style="width:100%;padding-bottom: 0px"></a>
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Pushmeet_Kohli"><a href="https://research.google/people/pushmeet-kohli/" target="_blank" rel="noopener noreferrer">Pushmeet Kohli</a></h2>
              <p>Google DeepMind</p>
              <div class="social">
			    <!--
				<a href=""><i class="fa fa-linkedin"></i></a>
                <a href=""><i class="fa fa-twitter"></i></a>
                <a href=""><i class="fa fa-facebook"></i></a>
                <a href=""><i class="fa fa-google-plus"></i></a>
                -->
              </div>
              <p>Bio:<br>Dr. Pushmeet Kohli, Vice President of Research (AI for Science, Reliable and Responsible AI) leads the science program at Google DeepMind, which uses AI to help accelerate scientific progress in areas ranging from genomics to quantum chemistry. <br><br>Pushmeet's team is responsible for AlphaFold, an AI system for predicting the 3D structure of proteins. The AlphaFold paper is one of most cited AI biology papers ever, with over 15,000 citations. His team is also working on AI systems for materials discovery and nuclear fusion. <br><br>Before working at Google DeepMind, Pushmeet spent 10 years in research for Microsoft, rising to the director of research at Microsoft’s Cognition group. Pushmeet has won a number of awards including the British Machine Vision Association’s “Sullivan Doctoral Thesis Award”, and is a member of the Association for Computing Machinery's (ACM) Distinguished Speaker Program. <br><br>He also leads research to ensure AI systems are safe, and was the UK government’s nominee for the Responsible AI working group as part of the <a href="https://gpai.ai/projects/responsible-ai/" target="_blank" rel="noopener noreferrer">Global partnership on AI</a>. On Google DeepMind's Reliable and Responsible AI team, Pushmeet led on <a href="https://deepmind.google/technologies/synthid/" target="_blank" rel="noopener noreferrer">SynthID</a>, a tool for watermarking and identifying AI-generated images.</p>
              <p>Keynote Title:<br>TBD.</p>
			  <p>Keynote Abstract:<br>TBD.</p>
			  <!-- <a href="">[Talk Video]</a> -->
            </div>
          </div>  
        </div>
		
		<p></p>

        <div class="row">
          <div class="col-md-6">
            <a href="https://www.microsoft.com/en-us/research/people/lijuanw/" target="_blank" rel="noopener noreferrer"><img src="img/speakers/Lijuan_Wang.png" alt="Lijuan Wang" class="img-fluid" style="width:100%;padding-bottom: 0px"></a>
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Lijuan_Wang"><a href="https://www.microsoft.com/en-us/research/people/lijuanw/" target="_blank" rel="noopener noreferrer">Lijuan Wang</a></h2>
              <p>Microsoft GenAI</p>
              <div class="social">

			    <!--
				<a href=""><i class="fa fa-linkedin"></i></a>
                <a href=""><i class="fa fa-twitter"></i></a>
                <a href=""><i class="fa fa-facebook"></i></a>
                <a href=""><i class="fa fa-google-plus"></i></a>
                -->

              </div>
              <p>Bio:<br>Dr. Lijuan Wang serves as a Principal Researcher and Research Manager, leading a multimodal generative AI research group within Microsoft GenAI. After earning her PhD from Tsinghua University, China, she began her tenure at Microsoft Research Asia in 2006 and later joined Microsoft Research in Redmond in 2016. Her research primarily focuses on multimodal understanding and generation, encompassing a wide range of areas from 3D talking heads to vision-language pre-training, vision foundation models and image/video generation. As a pivotal contributor in vision-language pretraining, image captioning, and object detection, her work has been integral to the development of various Microsoft products, including Cognitive Services and Office 365. Her recent explorations into GPT-4V's advanced capabilities, contributions to the development of DALL-E 3, and work on multimodal agents have garnered significant attention.</p>
              <p>Keynote Title:<br>Recent Advances in Multimodal Foundation Models.</p>
			  <p>Keynote Abstract:<br>Humans interact with the world through multiple modalities, naturally synchronizing and integrating diverse information. A key goal in artificial intelligence is to develop algorithms capable of understanding and generating multimodal content. Research encompasses a broad range of tasks, from visual understanding (including image classification, image-text retrieval, image captioning, visual question answering, object detection, and various segmentation tasks) to visual generation (such as text-to-image and text-to-video generation). Recent advancements have shown significant improvements in model capabilities and versatility, novel benchmarks for emergent capabilities, and a trend toward integrating understanding and generation. The computer vision community is now emphasizing the development of general-purpose vision foundation models, influenced by the success of large-scale pre-training and large language models. These efforts are moving from specialized models to versatile general-purpose assistants. This talk will explore cutting-edge learning and application strategies for multimodal foundation models. Topics include learning models for multimodal understanding and generation, benchmarking these models to evaluate emergent abilities in understanding and generation tasks, and developing advanced systems and agents based on vision foundation models.</p>

			  <!-- <p>Keynote Abstract:<br>TBD.</p> -->
			  <!-- <a href="">[Talk Video]</a> -->

            </div>
          </div>  
        </div>
<!--		
		<p></p>

        <div class="row">
          <div class="col-md-6">
            <a href="https://jiajunwu.com/" target="_blank" rel="noopener noreferrer"><img src="img/speakers/Jiajun_Wu.jpg" alt="Jiajun Wu" class="img-fluid" style="width:100%;padding-bottom: 0px"></a>
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Jiajun_Wu"><a href="https://jiajunwu.com/" target="_blank" rel="noopener noreferrer">Jiajun Wu</a></h2>
              <p>Stanford University</p>
              <div class="social">
-->
			    <!--
				<a href=""><i class="fa fa-linkedin"></i></a>
                <a href=""><i class="fa fa-twitter"></i></a>
                <a href=""><i class="fa fa-facebook"></i></a>
                <a href=""><i class="fa fa-google-plus"></i></a>
                -->
<!--
              </div>
              <p>Bio:<br>Jiajun Wu is an Assistant Professor of Computer Science at Stanford University, working on computer vision, machine learning, and computational cognitive science. Before joining Stanford, he was a Visiting Faculty Researcher at Google Research. He received his PhD in Electrical Engineering and Computer Science from the Massachusetts Institute of Technology. Wu's research has been recognized through the AFOSR Young Investigator Research Program (YIP), the ACM Doctoral Dissertation Award Honorable Mention, the AAAI/ACM SIGAI Doctoral Dissertation Award, the MIT George M. Sprowls PhD Thesis Award in Artificial Intelligence and Decision-Making, the 2020 Samsung AI Researcher of the Year, the IROS Best Paper Award on Cognitive Robotics, and faculty research awards from JPMC, Samsung, Amazon, and Meta.</p>
              <p>Keynote Title:<br>Concept Learning Across Domains and Modalities.</p>
-->
			  <!-- <p>Keynote Abstract:<br>TBD.</p> --> 
			  <!-- <a href="">[Talk Video]</a> -->
<!--
            </div>
          </div>  
        </div>
-->
<!--		
		<p></p>

        <div class="row">
          <div class="col-md-6">
            <a href="http://www.stat.ucla.edu/~sczhu/" target="_blank" rel="noopener noreferrer"><img src="img/speakers/Song-Chun_Zhu.jpg" alt="Song-Chun Zhu" class="img-fluid" style="width:100%;padding-bottom: 0px"></a>
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Song-Chun_Zhu"><a href="http://www.stat.ucla.edu/~sczhu/" target="_blank" rel="noopener noreferrer">Song-Chun Zhu</a></h2>
              <p>UCLA</p>
              <div class="social">
-->
			    <!--
				<a href=""><i class="fa fa-linkedin"></i></a>
                <a href=""><i class="fa fa-twitter"></i></a>
                <a href=""><i class="fa fa-facebook"></i></a>
                <a href=""><i class="fa fa-google-plus"></i></a>
                -->
<!--
              </div>
              <p>Bio:<br>Prof. Song-Chun Zhu is a Professor in the statistics and computer science department at UCLA and is the director of the Center for Vision, Cognition, Learning and Autonomy. He has authored more than 300 papers in computer vision, statistical modeling and learning, cognitive science, natural language and situated dialogue, and robot autonomy, and commonsense reasoning.</p>
              <p>Keynote Title:<br>TBD.</p>
			  <p>Keynote Abstract:<br>TBD.</p> 
-->
			  <!-- <a href="">[Talk Video]</a> -->
<!--
            </div>
          </div>  
        </div>
-->		
		


      </div>

    </section>

  </main>


  <!--==========================
    Footer
  ============================-->
  <footer id="footer">
  <!--
    <div class="footer-top">
      <div class="container">
        <div class="row">

          <div class="col-lg-3 col-md-6 footer-info">
            <img src="img/logo.png" alt="TheEvenet">
            <p>In alias aperiam. Placeat tempore facere. Officiis voluptate ipsam vel eveniet est dolor et totam porro. Perspiciatis ad omnis fugit molestiae recusandae possimus. Aut consectetur id quis. In inventore consequatur ad voluptate cupiditate debitis accusamus repellat cumque.</p>
          </div>

          <div class="col-lg-3 col-md-6 footer-links">
            <h4>Useful Links</h4>
            <ul>
              <li><i class="fa fa-angle-right"></i> <a href="#">Home</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">About us</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Services</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Terms of service</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Privacy policy</a></li>
            </ul>
          </div>

          <div class="col-lg-3 col-md-6 footer-links">
            <h4>Useful Links</h4>
            <ul>
              <li><i class="fa fa-angle-right"></i> <a href="#">Home</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">About us</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Services</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Terms of service</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Privacy policy</a></li>
            </ul>
          </div>

          <div class="col-lg-3 col-md-6 footer-contact">
            <h4>Contact Us</h4>
            <p>
              A108 Adam Street <br>
              New York, NY 535022<br>
              United States <br>
              <strong>Phone:</strong> +1 5589 55488 55<br>
              <strong>Email:</strong> info@example.com<br>
            </p>

            <div class="social-links">
              <a href="#" class="twitter"><i class="fa fa-twitter"></i></a>
              <a href="#" class="facebook"><i class="fa fa-facebook"></i></a>
              <a href="#" class="instagram"><i class="fa fa-instagram"></i></a>
              <a href="#" class="google-plus"><i class="fa fa-google-plus"></i></a>
              <a href="#" class="linkedin"><i class="fa fa-linkedin"></i></a>
            </div>

          </div>

        </div>
      </div>
    </div>
	-->
	
    <div class="container">
      <div class="copyright">
        &copy; Copyright <strong>TheEvent & MAR 2024</strong>. All Rights Reserved
      </div>
      <div class="credits">
        <!--
          All the links in the footer should remain intact.
          You can delete the links only if you purchased the pro version.
          Licensing information: https://bootstrapmade.com/license/
          Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/buy/?theme=TheEvent
        -->
        Designed by <a href="https://bootstrapmade.com/" target="_blank" rel="noopener noreferrer">BootstrapMade</a>
      </div>
    </div>
  </footer><!-- #footer -->

  <a href="#" class="back-to-top"><i class="fa fa-angle-up"></i></a>

  <!-- JavaScript Libraries -->
  <script src="lib/jquery/jquery.min.js"></script>
  <script src="lib/jquery/jquery-migrate.min.js"></script>
  <script src="lib/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="lib/easing/easing.min.js"></script>
  <script src="lib/superfish/hoverIntent.js"></script>
  <script src="lib/superfish/superfish.min.js"></script>
  <script src="lib/wow/wow.min.js"></script>
  <script src="lib/venobox/venobox.min.js"></script>
  <script src="lib/owlcarousel/owl.carousel.min.js"></script>

  <!-- Contact Form JavaScript File -->
  <script src="contactform/contactform.js"></script>

  <!-- Template Main Javascript File -->
  <script src="js/main.js"></script>
</body>

</html>
