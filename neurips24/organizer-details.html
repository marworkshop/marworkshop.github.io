<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>MAR 2024 - Multimodal Algorithmic Reasoning</title>
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <meta content="" name="keywords">
  <meta content="" name="description">

  <!-- Favicons -->
  <link href="img/cvpr24.png" rel="icon">
  <link href="img/cvpr24.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,700,700i|Raleway:300,400,500,700,800" rel="stylesheet">

  <!-- Bootstrap CSS File -->
  <link href="lib/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Libraries CSS Files -->
  <link href="lib/font-awesome/css/font-awesome.min.css" rel="stylesheet">
  <link href="lib/animate/animate.min.css" rel="stylesheet">
  <link href="lib/venobox/venobox.css" rel="stylesheet">
  <link href="lib/owlcarousel/assets/owl.carousel.min.css" rel="stylesheet">

  <!-- Main Stylesheet File -->
  <link href="css/style.css" rel="stylesheet">

  <!-- =======================================================
    Theme Name: TheEvent
    Theme URL: https://bootstrapmade.com/theevent-conference-event-bootstrap-template/
    Author: BootstrapMade.com
    License: https://bootstrapmade.com/license/
  ======================================================= -->
</head>

<body>

  <!--==========================
    Header
  ============================-->
  <header id="header" class="header-fixed">
    <div class="container">

      <div id="logo" class="pull-left">
        <!-- Uncomment below if you prefer to use a text logo -->
        <!-- <h1><a href="#main">C<span>o</span>nf</a></h1>-->
        <!-- <a href="index.html#intro" class="scrollto"><img src="img/logo.png" alt="" title=""></a> -->
        <a href="index.html#intro" class="scrollto"><p style="font-size:30px;color:white">MAR 2024</p></a>
      </div>

      <nav id="nav-menu-container">
        <ul class="nav-menu">
          <li class="menu-active"><a href="index.html#intro">Home</a></li>
          <li><a href="index.html#about">About</a></li>
          <li><a href="index.html#speakers">Speakers</a></li>
          <li><a href="index.html#schedule">Schedule</a></li>
		  <li><a href="index.html#submission">Submission</a></li>
		  <!--<li><a href="index.html#accepted_work">Accepted Work</a></li>-->
          <li><a href="index.html#venue">Venue</a></li>
          <!-- <li><a href="#hotels">Hotels</a></li> -->
          <!-- <li><a href="#gallery">Gallery</a></li> -->
          <li><a href="index.html#supporters">Sponsor</a></li>
          <!-- <li><a href="#contact">Contact</a></li> -->
          <li><a href="index.html#organizers">Contact</a></li>
		  <li><a href="../index.html">Past Workshops</a></li>
		  <li><a class="btn navbar-btn mx-2 text-white btn-outline-light" href="https://cmt3.research.microsoft.com/MARNIPS2024" target="_blank" rel="noopener noreferrer">CMT</a></li>
		  <!-- <li><a class="btn navbar-btn mx-2 text-white btn-outline-light" href="https://eval.ai/web/challenges/challenge-page/2088/overview" target="_blank" rel="noopener noreferrer">Challenge</a></li> -->
		  <!-- <li><a class="btn navbar-btn mx-2 text-white btn-outline-light" href="https://smartdataset.github.io/smart101/" target="_blank" rel="noopener noreferrer">SMART-101</a></li> -->
          <!-- <li><a href="../index.html#VASBSD_series">VASBSD Series</a></li> -->
          <!-- <li class="buy-tickets"><a href="#buy-tickets">Buy Tickets</a></li> -->
        </ul>
      </nav><!-- #nav-menu-container -->
    </div>
  </header><!-- #header -->

  <main id="main" class="main-page">

    <!--==========================
      Organizer Details Section
    ============================-->
    <section id="organizers-details" class="wow fadeIn">
      <div class="container">
        <div class="section-header">
          <h2>Organizer Details</h2>
          <!-- <p>TBD</p> -->
        </div>

		<div class="row">
          <div class="col-md-6">
            <a href="http://users.cecs.anu.edu.au/~cherian/" target="_blank" rel="noopener noreferrer"><img src="img/organizers/Anoop_Cherian.jpg" alt="Anoop Cherian" class="img-fluid" style="width:55%;padding-bottom: 0px"></a>
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Anoop_Cherian"><a href="http://users.cecs.anu.edu.au/~cherian/" target="_blank" rel="noopener noreferrer">Anoop Cherian</a></h2>
              <p>Mitsubishi Electric Research Laboratories (MERL)</p>
              <div class="social">
                <!-- <a href=""><i class="fa fa-twitter"></i></a> -->
                <!-- <a href=""><i class="fa fa-facebook"></i></a> -->
                <!-- <a href=""><i class="fa fa-google-plus"></i></a> -->
                <a href="https://www.linkedin.com/in/anoop-cherian-4678a04/" target="_blank" rel="noopener noreferrer"><i class="fa fa-linkedin"></i></a>
              </div>
              <p>Bio:<br>Dr. Anoop Cherian is a Senior Principal Research Scientist with Mitsubishi Electric Research Labs (MERL) in Cambridge, MA and an adjunct Senior Lecturer with the Australian National University (ANU), Canberra, Australia. He was a former Research Fellow with the Australian Center for Robotic Vision (ACRV) at ANU, and a Postdoctoral Fellow with the LEAR project team at Inria, Grenoble, France. Anoop received his MS and PhD degrees in 2010 and 2013 respectively from the University of Minnesota, Minneapolis, and B.Tech from National Institute of Technology, Calicut, India. Anoop has broad interests in the areas of multimodal and embodied AI, neuro-symbolic reasoning, generative models, robotics, and optimization. Anoop has organized several workshops at computer vision venues in the past, including the Multimodal Algorithmic Reasoning Workshop at CVPR 2024, the Vision-and-Language Algorithmic Reasoning Workshop at ICCV 2023, the Deep Declarative Networks Workshop at CVPR 2020, Tensor Methods in Computer Vision (TMCV) at CVPR 2017, Robotic Vision Summer School (RVSS 2017), and Visually Grounded Interaction and Language (VIGIL) at NeurIPS 2018, among others.</p>
            </div>
          </div>  
        </div>

		<p></p>

        <div class="row">
          <div class="col-md-6">
            <a href="http://www.merl.com/people/kpeng"  target="_blank" rel="noopener noreferrer"><img src="img/organizers/Kuan-Chuan_Peng.jpg" alt="Kuan-Chuan Peng" class="img-fluid" style="width:55%;padding-bottom: 0px"></a>
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Kuan-Chuan_Peng"><a href="http://www.merl.com/people/kpeng" target="_blank" rel="noopener noreferrer">Kuan-Chuan Peng</a></h2>
              <p>Mitsubishi Electric Research Laboratories (MERL)</p>
              <div class="social">
                <!-- <a href=""><i class="fa fa-twitter"></i></a> -->
                <a href="https://www.facebook.com/LynxPeng" target="_blank" rel="noopener noreferrer"><i class="fa fa-facebook"></i></a>
                <!-- <a href=""><i class="fa fa-google-plus"></i></a> -->
                <a href="https://www.linkedin.com/in/kuan-chuan-peng-8344817a/" target="_blank" rel="noopener noreferrer"><i class="fa fa-linkedin"></i></a>
              </div>
              <p>Bio:<br>Dr. Kuan-Chuan Peng is an IEEE Senior Member and a Principal Research Scientist at Mitsubishi Electric Research Labs (MERL) in Cambridge, MA. He received his Ph.D. degree in Electrical and Computer Engineering from Cornell University in 2016. He received a B.S. degree in Electrical Engineering and an M.S. degree in Computer Science from National Taiwan University in 2009 and 2012 respectively. His expertise includes domain adaptation, anomaly detection, attention modeling, and fundamental computer vision and machine learning problems. He organized: (1) 2024 Workshop on Anomaly Detection with Foundation Models in conjunction with IJCAI 2024. (2) 2024 Workshop on Multimodal Algorithmic Reasoning Workshop in conjunction with CVPR 2024. (3) 2023 Workshop on Vision-and-Language Algorithmic Reasoning in conjunction with ICCV 2023. (4) 2022 and 2024 Workshop on Artificial Intelligence with Biased or Scarce Data in conjunction with AAAI 2022 and 2024. (5) 2022 Workshop on Vision with Biased or Scarce Data in conjunction with ECCV 2022. (6) 2020, 2021, 2022, 2023, and 2024 Workshop on Fair, Data-Efficient and Trusted Computer Vision in conjunction with CVPR in 2020, 2021, 2022, 2023, and 2024. (7) 2020 and 2021 Workshop on Vision Applications and Solutions to Biased or Scarce Data in conjunction with WACV in 2020 and 2021. (8) 2018 and 2019 Workshop on Vision with Biased or Scarce Data in conjunction with CVPR in 2018 and 2019.</p>
            </div>
          </div>  
        </div>
		
		<p></p>
		
		<div class="row">
          <div class="col-md-6">
            <a href="https://www.merl.com/people/slohit" target="_blank" rel="noopener noreferrer"><img src="img/organizers/Suhas_Lohit.jpg" alt="Suhas Lohit" class="img-fluid" style="width:55%;padding-bottom: 0px"></a>
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Suhas_Lohit"><a href="https://www.merl.com/people/slohit" target="_blank" rel="noopener noreferrer">Suhas Lohit</a></h2>
              <p>Mitsubishi Electric Research Laboratories (MERL)</p>
              <div class="social">
                <!-- <a href=""><i class="fa fa-twitter"></i></a> -->
                <!-- <a href=""><i class="fa fa-facebook"></i></a> -->
                <!-- <a href=""><i class="fa fa-google-plus"></i></a> -->
                <a href="https://www.linkedin.com/in/suhaslohit/" target="_blank" rel="noopener noreferrer"><i class="fa fa-linkedin"></i></a>
              </div>
              <p>Bio:<br>Dr. Suhas Lohit is a Principal Research Scientist at Mitsubishi Electric Research Labs (MERL) in Cambridge, MA, USA. Before coming to MERL, Suhas received his Ph.D degree in Electrical Engineering from Arizona State University in 2019. His research interests include computer vision, computational imaging and deep learning. Recently, his research focus has been on creating hybrid model- and data-driven neural architectures for various applications in imaging and vision. He organized: (1) 2024 Workshop on Multimodal Algorithmic Reasoning Workshop in conjunction with CVPR 2024. (2) 2023 Workshop on Vision-and-Language Algorithmic Reasoning in conjunction with ICCV 2023.</p>
            </div>
          </div>  
        </div>
		
		<p></p>
		
		<div class="row">
          <div class="col-md-6">
            <a href="https://sites.google.com/view/hongluzhou/" target="_blank" rel="noopener noreferrer"><img src="img/organizers/Honglu_Zhou.png" alt="Honglu Zhou" class="img-fluid" style="width:55%;padding-bottom: 0px"></a>
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Honglu_Zhou"><a href="https://sites.google.com/view/hongluzhou/" target="_blank" rel="noopener noreferrer">Honglu Zhou</a></h2>
              <p>Salesforce Research</p>
              <div class="social">

                <!-- <a href=""><i class="fa fa-twitter"></i></a> -->
                <!-- <a href=""><i class="fa fa-facebook"></i></a> -->
                <!-- <a href=""><i class="fa fa-google-plus"></i></a> -->
                <a href="https://www.linkedin.com/in/honglu-zhou-21058a169/" target="_blank" rel="noopener noreferrer"><i class="fa fa-linkedin"></i></a>
              </div>
              <p>Bio:<br>Dr. Honglu Zhou is a Research Scientist at Salesforce Research. She received her Ph.D. degree at Rutgers University in the Computer Science Department, under the supervision of Prof. Mubbasir Kapadia. She was a member of the Intelligent Visual Interfaces Lab. Her work was published in numerous renowned conferences (CVPR, ICLR, ECCV, SIGIR, IJCAI) and journals (Springer The Visual Computer, IEEE Computer Graphics and Applications). Her work has been recognized with a Best Paper Award from CIKM 2022. She has done multiple internships and collaborated with researchers from Salesforce Research, NEC Laboratories America, Google YouTube, DeepMind, and Google Research. Honglu is passionate about next-generation machine intelligence. Her research interests lie in the areas of video understanding, machine reasoning, multimodality and generative models. Her goal is to tackle fundamental problems in high-level semantic analysis and reasoning by developing effective and efficient methods, especially for videos and human behavior sequences, so that machines are able to interpret and interact with the dynamic visual world that we humans live in. She organized the 2024 Workshop on Multimodal Algorithmic Reasoning Workshop in conjunction with CVPR 2024 and the 2023 Workshop on Vision-and-Language Algorithmic Reasoning in conjunction with ICCV 2023.</p>
            </div>
          </div>  
        </div>
		
		<p></p>
<!--	
		<div class="row">
          <div class="col-md-6">
            <a href="https://www.merl.com/people/chatterjee" target="_blank" rel="noopener noreferrer"><img src="img/organizers/Moitreya_Chatterjee.jpg" alt="Moitreya Chatterjee" class="img-fluid" style="width:55%;padding-bottom: 0px"></a>
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Moitreya_Chatterjee"><a href="https://www.merl.com/people/chatterjee" target="_blank" rel="noopener noreferrer">Moitreya Chatterjee</a></h2>
              <p>Mitsubishi Electric Research Laboratories (MERL)</p>
              <div class="social">
-->
                <!-- <a href=""><i class="fa fa-twitter"></i></a> -->
                <!-- <a href=""><i class="fa fa-facebook"></i></a> -->
				<!-- <a href=""><i class="fa fa-google-plus"></i></a> -->
<!--
                <a href="https://www.linkedin.com/in/moitreya-chatterjee-3937b863/" target="_blank" rel="noopener noreferrer"><i class="fa fa-linkedin"></i></a>
              </div>
              <p>Bio:<br>Dr. Moitreya Chatterjee is a Research Scientist at Mitsubishi Electric Research Labs (MERL) in Cambridge, MA. He received his Ph.D. degree in Electrical and Computer Engineering from the University of Illinois at Urbana-Champaign in 2022. Moitreya's research interests are in computer vision, and multimodal machine learning with a particular emphasis on learning from audio-visual data. His PhD work received the Joan and Lalit Bahl Fellowship and the Thomas and Margaret Huang Research Award. Earlier, he earned a M.S. degree in Computer Science from the University of Southern California (USC), during which he received an Outstanding Paper Award from the ACM International Conference on Multimodal Interaction (ICMI).</p>
            </div>
          </div>  
        </div>
		
		<p></p>
-->		
		<div class="row">
          <div class="col-md-6">
            <a href="http://www.mit.edu/~k2smith/" target="_blank" rel="noopener noreferrer"><img src="img/organizers/Kevin_Smith.jpg" alt="Kevin Smith" class="img-fluid" style="width:55%;padding-bottom: 0px"></a>
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Kevin_Smith"><a href="http://www.mit.edu/~k2smith/" target="_blank" rel="noopener noreferrer">Kevin Smith</a></h2>
              <p>MIT</p>
              <div class="social">
                <a href="https://twitter.com/realkevinsmith" target="_blank" rel="noopener noreferrer"><i class="fa fa-twitter"></i></a>
                <!-- <a href=""><i class="fa fa-facebook"></i></a> -->
                <!-- <a href=""><i class="fa fa-google-plus"></i></a> -->
                <a href="https://www.linkedin.com/in/kevin-smith-4636944/" target="_blank" rel="noopener noreferrer"><i class="fa fa-linkedin"></i></a>
              </div>
              <p>Bio:<br>Dr. Kevin Smith is a research scientist at the Computational Cognitive Sciences lab at MIT, whose research reverse engineers the processes people use to understand and reason about the physical world. His work has been recognized with a best paper award from Robotics: Science and Systems 2018, as well as for the best Perception & Action model at the Cognitive Sciences Society 2012. He has previously organized workshops and symposia at CVPR (the Workshop on Multimodal Algorithmic Reasoning, 2024), ICCV (the Workshop on Vision-and-Language Algorithmic Reasoning, 2023), NeurIPS (Modeling the Physical World: Perception, Learning, and Control), 2018; Physical Reasoning and Inductive Biases for the Real World, 2021), ECCV (First Challenge on Machine Visual Common Sense: Perception, Prediction, Planning, 2022), and the Cognitive Sciences Society (The Origins of Common Sense in Humans and Machines, 2020; Strategies and Representations in Physical Inference, 2018).</p>
            </div>
          </div>  
        </div>
		
		<p></p>
		
		<div class="row">
          <div class="col-md-6">
            <a href="https://www.merl.com/people/tmarks" target="_blank" rel="noopener noreferrer"><img src="img/organizers/Tim_Marks.jpg" alt="Tim Marks" class="img-fluid" style="width:55%;padding-bottom: 0px"></a>
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Tim_Marks"><a href="https://www.merl.com/people/tmarks" target="_blank" rel="noopener noreferrer">Tim Marks</a></h2>
              <p>Mitsubishi Electric Research Laboratories (MERL)</p>
              <div class="social">

                <!-- <a href=""><i class="fa fa-twitter"></i></a> -->
                <!-- <a href=""><i class="fa fa-facebook"></i></a> -->
                <!-- <a href=""><i class="fa fa-google-plus"></i></a> -->
                <a href="https://www.linkedin.com/in/tim-marks-063894132/" target="_blank" rel="noopener noreferrer"><i class="fa fa-linkedin"></i></a>
              </div>
              <p>Bio:<br>Dr. Tim Marks is a Senior Principal Research Scientist and Senior Team Leader at Mitsubishi Electric Research Labs (MERL) in Cambridge, MA. Prior to joining MERL in 2008, he did postdoctoral research in robotic Simultaneous Localization and Mapping in collaboration with NASA's Jet Propulsion Laboratory. His research at MERL spans a variety of areas in computer vision and machine learning, including face recognition under variations in pose and lighting, and robotic vision and touch-based registration for industrial automation. He organized the 2023 Workshop on Vision-and-Language Algorithmic Reasoning in conjunction with ICCV 2023 and the 2024 Workshop on Multimodal Algorithmic Reasoning in conjunction with CVPR 2024.</p>
            </div>
          </div>  
        </div>
		
		<p></p>
		
		<div class="row">
          <div class="col-md-6">
            <a href="https://www.niebles.net/" target="_blank" rel="noopener noreferrer"><img src="img/organizers/Juan_Carlos_Niebles.jpg" alt="Juan Carlos Niebles" class="img-fluid" style="width:55%;padding-bottom: 0px"></a>
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Juan_Carlos_Niebles"><a href="https://www.niebles.net/" target="_blank" rel="noopener noreferrer">Juan Carlos Niebles</a></h2>
              <p>Salesforce AI Research</p>
              <div class="social">

                <!-- <a href=""><i class="fa fa-twitter"></i></a> -->
                <!-- <a href=""><i class="fa fa-facebook"></i></a> -->
                <!-- <a href=""><i class="fa fa-google-plus"></i></a> -->
                <!-- <a href=""><i class="fa fa-linkedin"></i></a>-->
              </div>
              <p>Bio:<br>Dr. Juan Carlos Niebles received an Engineering degree in Electronics from Universidad del Norte (Colombia) in 2002, an M.Sc. degree in Electrical and Computer Engineering from University of Illinois at Urbana-Champaign in 2007, and a Ph.D. degree in Electrical Engineering from Princeton University in 2011. He is Research Director at Salesforce and Adjunct Professor of Computer Science at Stanford since 2021. He is co-Director of the Stanford Vision and Learning Lab. Before that, he was Associate Director of Research at the Stanford-Toyota Center for AI Research and a Senior Research Scientist at the Stanford AI Lab between 2015 and 2021. He was also an Associate Professor of Electrical and Electronic Engineering in Universidad del Norte (Colombia) between 2011 and 2019. His research interests are in computer vision and machine learning, with a focus on visual recognition and understanding of human actions and activities, objects, scenes, and events. He serves as Area Chair for CVPR, ICCV and ECCV, as well as Associate Editor for IEEE TPAMI. He is also a member of the AI Index Steering Committee and is the Curriculum Director for Stanford-AI4ALL. He is a recipient of a Google Faculty Research award (2015), the Microsoft Research Faculty Fellowship (2012), a Google Research award (2011) and a Fulbright Fellowship (2005).</p>
            </div>
          </div>  
        </div>
		
		<p></p>
		

<!--		
		<div class="row">
          <div class="col-md-6">
            <a href="https://www.linkedin.com/in/joanna-matthiesen-61a52a35/" target="_blank" rel="noopener noreferrer"><img src="img/organizers/Joanna_Matthiesen.jpg" alt="Joanna Matthiesen" class="img-fluid" style="width:55%;padding-bottom: 0px"></a>
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Joanna_Matthiesen"><a href="https://www.linkedin.com/in/joanna-matthiesen-61a52a35/" target="_blank" rel="noopener noreferrer">Joanna Matthiesen</a></h2>
              <p>Math Kangaroo USA, Association Kangourou sans Frontières, Notre Dame University</p>
              <div class="social">
-->
                <!-- <a href=""><i class="fa fa-twitter"></i></a> -->
                <!-- <a href="https://www.facebook.com/joanna.lasekmatthiesen/" target="_blank" rel="noopener noreferrer"><i class="fa fa-facebook"></i></a> -->
                <!-- <a href=""><i class="fa fa-google-plus"></i></a> -->
                <!-- <a href="https://www.linkedin.com/in/joanna-matthiesen-61a52a35" target="_blank" rel="noopener noreferrer"><i class="fa fa-linkedin"></i></a> -->
<!--
              </div>
              <p>Bio:<br>Joanna Matthiesen is the president, CEO and co-founder of Math Kangaroo USA, a nationwide nonprofit with a focus on promoting the love of mathematics among students of all ages. Her focus is to help students gain in their interest in mathematics and logic, and in turn, use their skills to make the world a better place. She is passionate about the use of Kangaroo problems to challenge students in both creative thinking and advanced intellectual development. She serves on the International Board of the Association Kangourou sans Frontières to oversee its governance, policy, and annual question selection. Previously, she spent 12 years as a mathematics instructor at various charter high schools in Chicago and Los Angeles, as well as Roosevelt University and City Colleges in Chicago. Joanna holds a Masters in Mathematics from Holy Cross University in Poland, and recently (2022) graduated with honors from the University of Notre Dame with an Executive Masters of Nonprofit Administration. She is fascinated with developing the use of AI in solving simple and complex math and logic problems.</p>
            </div>
          </div>  
        </div>
		
		<p></p>
-->		

		<div class="row">
          <div class="col-md-6">
            <a href="https://petar-v.com/" target="_blank" rel="noopener noreferrer"><img src="img/organizers/Petar_Velickovic.jpg" alt="Petar Velickovic" class="img-fluid" style="width:55%;padding-bottom: 0px"></a>
          </div>
          <div class="col-md-6">
            <div class="details">
              <h2 id="Petar_Velickovic"><a href="https://petar-v.com/" target="_blank" rel="noopener noreferrer">Petar Veličković</a></h2>
              <p>Google DeepMind</p>
              <div class="social">
                <a href="https://twitter.com/PetarV_93" target="_blank" rel="noopener noreferrer"><i class="fa fa-twitter"></i></a>
                  <!--<a href=""><i class="fa fa-facebook"></i></a>-->
                  <!--<a href=""><i class="fa fa-google-plus"></i></a>-->
                  <a href="https://www.linkedin.com/in/petarvelickovic" target="_blank" rel="noopener noreferrer"><i class="fa fa-linkedin"></i></a>
              </div>
              <p>Bio:<br>Dr. Petar Veličković is a Staff Research Scientist at <a href="https://www.deepmind.com/" target="_blank" rel="noopener noreferrer">Google DeepMind</a>, Affiliated Lecturer at the <a href="https://www.cam.ac.uk/" target="_blank" rel="noopener noreferrer">University of Cambridge</a>, and an Associate of <a href="https://www.clarehall.cam.ac.uk/" target="_blank" rel="noopener noreferrer">Clare Hall, Cambridge</a>. He holds a PhD in Computer Science from the <a href="https://www.cam.ac.uk/" target="_blank" rel="noopener noreferrer">University of Cambridge</a> (<a href="https://www.trin.cam.ac.uk/" target="_blank" rel="noopener noreferrer">Trinity College</a>), obtained under the supervision of <a href="https://www.cst.cam.ac.uk/~pl219" target="_blank" rel="noopener noreferrer">Pietro Liò</a>. His research concerns <a href="https://www.youtube.com/watch?v=9cxhvQK9ALQ" target="_blank" rel="noopener noreferrer">geometric deep learning</a>—devising neural network architectures that respect the invariances and symmetries in data (a topic he has co-written a <a href="https://geometricdeeplearning.com/" target="_blank" rel="noopener noreferrer">proto-book</a> about). For his contributions, he is recognised as an <a href="https://ellis.eu/" target="_blank" rel="noopener noreferrer">ELLIS</a> Scholar in the <a href="https://ellis.eu/programs/geometric-deep-learning" target="_blank" rel="noopener noreferrer">Geometric Deep Learning Program</a>. Particularly, he focuses on <a href="https://www.youtube.com/watch?v=uF53xsT7mjc" target="_blank" rel="noopener noreferrer">graph representation learning</a> and its applications in <a href="https://www.youtube.com/watch?v=U2vybDdoDAQ" target="_blank" rel="noopener noreferrer">algorithmic reasoning</a> (featured in <a href="https://venturebeat.com/2021/09/10/deepmind-aims-to-marry-deep-learning-and-classic-algorithms/" target="_blank" rel="noopener noreferrer">Venture</a><a href="https://venturebeat.com/2021/10/12/deepmind-is-developing-one-algorithm-to-rule-them-all/" target="_blank" rel="noopener noreferrer">Beat</a>). He is the first author of <a href="http://petar-v.com/GAT/" target="_blank" rel="noopener noreferrer">Graph Attention Networks</a>—a popular convolutional layer for graphs—and <a href="https://openreview.net/forum?id=rklz9iAcKQ" target="_blank" rel="noopener noreferrer">Deep Graph Infomax</a>—a popular self-supervised learning pipeline for graphs (featured in <a href="https://www.zdnet.com/article/google-brain-microsoft-plumb-the-mysteries-of-networks-with-ai/" target="_blank" rel="noopener noreferrer">ZDNet</a>). His research has been used in <a href="https://deepmind.com/blog/article/traffic-prediction-with-advanced-graph-neural-networks" target="_blank" rel="noopener noreferrer">substantially improving travel-time predictions</a> in <a href="https://blog.google/products/maps/google-maps-101-how-ai-helps-predict-traffic-and-determine-routes/" target="_blank" rel="noopener noreferrer">Google Maps</a> (featured in the <a href="https://www.cnbc.com/2020/09/03/covid-19-forced-google-maps-to-change-how-it-predicts-traffic.html" target="_blank" rel="noopener noreferrer">CNBC</a>, <a href="https://www.engadget.com/google-maps-deep-mind-ai-accuracy-140005698.html" target="_blank" rel="noopener noreferrer">Endgadget</a>, <a href="https://deepmind.google/technologies/synthid/https://venturebeat.com/2020/09/03/deepmind-claims-its-ai-improved-google-maps-travel-time-estimates-by-up-to-50/oreferrer">VentureBeat</a>, <a href="https://www.cnet.com/news/heres-how-google-maps-uses-ai-to-predict-traffic-and-calculate-routes/" target="_blank" rel="noopener noreferrer">CNET</a>, the <a href="https://www.theverge.com/2020/9/3/21419632/how-google-maps-predicts-traffic-eta-ai-machine-learning-deepmind" target="_blank" rel="noopener noreferrer">Verge</a> and <a href="https://www.zdnet.com/article/google-maps-and-deepmind-enhance-ai-capabilities-to-improve-route-calculations/" target="_blank" rel="noopener noreferrer">ZDNet</a>), and <a href="https://www.nature.com/articles/s41586-021-04086-x" target="_blank" rel="noopener noreferrer">guiding intuition of mathematicians</a> towards new top-tier <a href="https://arxiv.org/abs/2111.15161" target="_blank" rel="noopener noreferrer">theorems and conjectures</a> (featured in <a href="https://www.nature.com/articles/d41586-021-03593-1" target="_blank" rel="noopener noreferrer">Nature</a>, <a href="https://www.science.org/content/blog-post/ai-aid-human-intuition" target="_blank" rel="noopener noreferrer">Science</a>, <a href="https://www.quantamagazine.org/deepmind-machine-learning-becomes-a-mathematical-collaborator-20220215/" target="_blank" rel="noopener noreferrer">Quanta Magazine</a>, <a href="https://www.newscientist.com/article/2299564-deepmind-ai-collaborates-with-humans-on-two-mathematical-breakthroughs/" target="_blank" rel="noopener noreferrer">New Scientist</a>, <a href="https://www.independent.co.uk/life-style/gadgets-and-tech/ai-artificial-intelligence-maths-deepmind-b1967817.html" target="_blank" rel="noopener noreferrer">The Independent</a>, <a href="https://news.sky.com/story/mathematicians-hail-breakthrough-in-using-ai-to-suggest-new-theorems-12483934" target="_blank" rel="noopener noreferrer">Sky News</a>, <a href="https://www.thetimes.co.uk/article/deepminds-artificial-intelligence-software-helps-mathematicians-pinpoint-patterns-3c5cwmmdt" target="_blank" rel="noopener noreferrer">The Sunday Times</a>, <a href="https://www.repubblica.it/tecnologia/2022/07/04/news/intuizione_aumentata_cosi_lintelligenza_artificiale_aiuta_i_matematici_nelle_loro_scoperte-354320142/" target="_blank" rel="noopener noreferrer">la Repubblica</a> and <a href="https://theconversation.com/mathematical-discoveries-take-intuition-and-creativity-and-now-a-little-help-from-ai-172900" target="_blank" rel="noopener noreferrer">The Conversation</a>).</p>
            </div>
          </div>  
        </div>


      </div>

    </section>

  </main>


  <!--==========================
    Footer
  ============================-->
  <footer id="footer">
  <!--
    <div class="footer-top">
      <div class="container">
        <div class="row">

          <div class="col-lg-3 col-md-6 footer-info">
            <img src="img/logo.png" alt="TheEvenet">
            <p>In alias aperiam. Placeat tempore facere. Officiis voluptate ipsam vel eveniet est dolor et totam porro. Perspiciatis ad omnis fugit molestiae recusandae possimus. Aut consectetur id quis. In inventore consequatur ad voluptate cupiditate debitis accusamus repellat cumque.</p>
          </div>

          <div class="col-lg-3 col-md-6 footer-links">
            <h4>Useful Links</h4>
            <ul>
              <li><i class="fa fa-angle-right"></i> <a href="#">Home</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">About us</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Services</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Terms of service</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Privacy policy</a></li>
            </ul>
          </div>

          <div class="col-lg-3 col-md-6 footer-links">
            <h4>Useful Links</h4>
            <ul>
              <li><i class="fa fa-angle-right"></i> <a href="#">Home</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">About us</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Services</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Terms of service</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Privacy policy</a></li>
            </ul>
          </div>

          <div class="col-lg-3 col-md-6 footer-contact">
            <h4>Contact Us</h4>
            <p>
              A108 Adam Street <br>
              New York, NY 535022<br>
              United States <br>
              <strong>Phone:</strong> +1 5589 55488 55<br>
              <strong>Email:</strong> info@example.com<br>
            </p>

            <div class="social-links">
              <a href="#" class="twitter"><i class="fa fa-twitter"></i></a>
              <a href="#" class="facebook"><i class="fa fa-facebook"></i></a>
              <a href="#" class="instagram"><i class="fa fa-instagram"></i></a>
              <a href="#" class="google-plus"><i class="fa fa-google-plus"></i></a>
              <a href="#" class="linkedin"><i class="fa fa-linkedin"></i></a>
            </div>

          </div>

        </div>
      </div>
    </div>
	-->
	
    <div class="container">
      <div class="copyright">
        &copy; Copyright <strong>TheEvent & MAR 2024</strong>. All Rights Reserved
      </div>
      <div class="credits">
        <!--
          All the links in the footer should remain intact.
          You can delete the links only if you purchased the pro version.
          Licensing information: https://bootstrapmade.com/license/
          Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/buy/?theme=TheEvent
        -->
        Designed by <a href="https://bootstrapmade.com/" target="_blank" rel="noopener noreferrer">BootstrapMade</a>
      </div>
    </div>
  </footer><!-- #footer -->

  <a href="#" class="back-to-top"><i class="fa fa-angle-up"></i></a>

  <!-- JavaScript Libraries -->
  <script src="lib/jquery/jquery.min.js"></script>
  <script src="lib/jquery/jquery-migrate.min.js"></script>
  <script src="lib/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="lib/easing/easing.min.js"></script>
  <script src="lib/superfish/hoverIntent.js"></script>
  <script src="lib/superfish/superfish.min.js"></script>
  <script src="lib/wow/wow.min.js"></script>
  <script src="lib/venobox/venobox.min.js"></script>
  <script src="lib/owlcarousel/owl.carousel.min.js"></script>

  <!-- Contact Form JavaScript File -->
  <script src="contactform/contactform.js"></script>

  <!-- Template Main Javascript File -->
  <script src="js/main.js"></script>
</body>

</html>
